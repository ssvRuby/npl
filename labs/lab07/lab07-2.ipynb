{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.2.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.2 (default, Sep 21 2017 18:29:43)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# SPARK_HOME=\"/Users/ssv/spark-2.2.0-bin-hadoop2.7\"\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--packages com.databricks:spark-csv_2.10:1.2.0 pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='python3'\n",
    "# os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"SPARK_HOME\"]='/Users/ssv/spark-2.2.0-bin-hadoop2.7'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.4-src.zip'))\n",
    "os.environ[\"PYSPARK_PYTHON\"] = 'python3'\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/Users/ssv/newprolab/labs/lab07/DO_record_per_line.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ru = df.filter(df.lang == 'ru')\n",
    "df_ru = df.filter(df.lang == 'es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------+\n",
      "|                 cat|                desc| id|lang|                name|provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------+\n",
      "|                    |A través de difer...| 59|  es|El ABC del empren...|Coursera|\n",
      "|2/biology_life_sc...|Aprenderemos cómo...|124|  es|Pensamiento Cient...|Coursera|\n",
      "+--------------------+--------------------+---+----+--------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ru.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"desc\", outputCol=\"words\", minTokenLength=3, \\\n",
    "                                                 gaps=True, pattern='((?U)[^\\w]|[+])')\n",
    "r_tokenize_df_ru = regexTokenizer.transform(df_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # “danish”, “dutch”, “english”, “finnish”, “french”, “german”, “hungarian”, “italian”, \n",
    "# # “norwegian”, “portuguese”, “russian”, “spanish”, “swedish” and “turkish”\n",
    "\n",
    "# remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "# remover.loadDefaultStopWords('russian')\n",
    "# remover.transform(tokenize_df_ru).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "featurizedData = hashingTF.transform(r_tokenize_df_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescaledData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "normalizer_l2 = Normalizer(inputCol=\"features\", outputCol=\"normalized_features\", p=2)\n",
    "\n",
    "normalized_rescaledData = normalizer_l2.transform(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cat: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- provider: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- normalized_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normalized_rescaledData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(a, b):\n",
    "    return a.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(normalized_features=SparseVector(10000, {249: 0.0908, 522: 0.0685, 527: 0.058, 643: 0.0698, 709: 0.0948, 808: 0.0024, 841: 0.0711, 909: 0.0607, 993: 0.0828, 1047: 0.0601, 1072: 0.079, 1096: 0.0456, 1203: 0.0044, 1236: 0.0784, 1312: 0.0079, 1448: 0.0379, 1606: 0.0015, 1930: 0.0526, 1959: 0.0078, 2080: 0.008, 2090: 0.0504, 2171: 0.0771, 2249: 0.0504, 2369: 0.1091, 2384: 0.0771, 2460: 0.0069, 2726: 0.0414, 2739: 0.098, 2850: 0.0315, 2906: 0.05, 2922: 0.1266, 3084: 0.0539, 3138: 0.0078, 3270: 0.5431, 3375: 0.0771, 3429: 0.0354, 3586: 0.0053, 3855: 0.0077, 3921: 0.0077, 4213: 0.1175, 4306: 0.0948, 4423: 0.0537, 4723: 0.0644, 4758: 0.1462, 5034: 0.026, 5260: 0.0634, 5262: 0.0041, 5300: 0.0609, 5314: 0.0358, 5373: 0.0075, 5529: 0.0998, 5612: 0.0081, 5641: 0.0908, 5660: 0.0644, 5710: 0.0145, 5772: 0.0741, 5786: 0.0078, 5798: 0.0472, 5820: 0.0885, 5872: 0.0529, 5990: 0.1091, 6105: 0.4655, 6119: 0.1709, 6215: 0.0283, 6256: 0.0681, 6276: 0.1017, 6306: 0.0805, 6316: 0.1123, 6324: 0.0083, 6443: 0.0764, 6469: 0.0864, 6526: 0.0234, 6632: 0.0637, 6812: 0.0303, 6888: 0.0074, 6959: 0.057, 6969: 0.0076, 7075: 0.0637, 7352: 0.0079, 7496: 0.082, 7616: 0.0551, 7848: 0.0345, 8189: 0.0524, 8194: 0.04, 8333: 0.007, 8382: 0.0627, 8493: 0.0079, 8599: 0.0557, 8696: 0.009, 8698: 0.1975, 8734: 0.0828, 8850: 0.0685, 8995: 0.0797, 9165: 0.014, 9241: 0.0777, 9282: 0.0908, 9320: 0.0822, 9455: 0.0518, 9524: 0.038, 9542: 0.0784, 9614: 0.0373, 9677: 0.0711, 9784: 0.0805, 9903: 0.082, 9926: 0.0681, 9976: 0.0601, 9980: 0.0828}))]"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cd = normalized_rescaledData.filter('id = 16704').select('normalized_features').take(1)\n",
    "#cd = normalized_rescaledData.filter('id = 13702').select('normalized_features').take(1)\n",
    "cd = normalized_rescaledData.filter('id = 11556').select('normalized_features').take(1)\n",
    "cd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "choosen_doc = cd[0].normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = normalized_rescaledData.select(normalized_rescaledData.id, \n",
    "                                     normalized_rescaledData.normalized_features.alias('nf'),\n",
    "                                     normalized_rescaledData.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = fdf.rdd.map(lambda x: (x[0], float(x[1].dot(choosen_doc)), x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_df = spark.createDataFrame(rdd, schema=['id','cm', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+--------------------+\n",
      "|   id|                 cm|                name|\n",
      "+-----+-------------------+--------------------+\n",
      "|11556|                1.0|Aprendizaje Colab...|\n",
      "|10384|0.26708008906411607|Marketing Digital...|\n",
      "|16488| 0.2520262532288868|Aprendizaje basad...|\n",
      "|  468|0.22799514721585262|Tecnologías de in...|\n",
      "|22710| 0.2029379209171897|Aplicaciones crea...|\n",
      "|13461| 0.1984825271366596|TIC para la enseñ...|\n",
      "|21707|0.18499702189474398|Tecnologías para ...|\n",
      "|19330| 0.1822903277838439|Introducción al m...|\n",
      "|23357|0.18044046069955805|Diseña Cursos Din...|\n",
      "|10447| 0.1743993046639929|Mejores calificac...|\n",
      "| 9465|0.15857582159660835|Desarrolla tu esp...|\n",
      "+-----+-------------------+--------------------+\n",
      "only showing top 11 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cos_df.orderBy(['cm', 'name', 'id'], ascending=[0, 1, 1]).show(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11556, [10384, 16488, 468, 22710, 13461, 21707, 19330, 23357, 10447, 9465]]\n"
     ]
    }
   ],
   "source": [
    "res_lst = cos_df.select('id').orderBy(['cm', 'name', 'id'], ascending=[0, 1, 1]).take(11)\n",
    "\n",
    "courses = []\n",
    "courses.append(res_lst[0].id)\n",
    "\n",
    "recommended = []\n",
    "for i in range(1,11):\n",
    "    recommended.append(res_lst[i].id)\n",
    "\n",
    "courses.append(recommended)\n",
    "print(courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en\n",
    "[23126, [14760, 13665, 13782, 20638, 24419, 15909, 2724, 25782, 17499, 13348]]\n",
    "[21617, [21609, 21673, 21081, 21616, 19417, 22298, 380, 8110, 116, 16971]]\n",
    "\n",
    "\n",
    "es\n",
    "[16627, [11431, 17961, 17964, 5687, 12247, 16694, 5558, 12660, 11575, 9563]]\n",
    "[11556, [10384, 16488, 468, 22710, 13461, 21707, 19330, 23357, 10447, 9465]]\n",
    "\n",
    "ru\n",
    "[16704, [1219, 1327, 20362, 1228, 55, 26980, 1236, 1247, 1365, 913]]\n",
    "[13702, [864, 21079, 1111, 792, 1410, 8123, 8313, 1041, 1033, 1396]]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = {\n",
    "    '23126' : [14760, 13665, 13782, 20638, 24419, 15909, 2724, 25782, 17499, 13348],\n",
    "    '21617' : [21609, 21673, 21081, 21616, 19417, 22298, 380, 8110, 116, 16971],\n",
    "    '16627' : [11431, 17961, 17964, 5687, 12247, 16694, 5558, 12660, 11575, 9563],\n",
    "    '11556' : [10384, 16488, 468, 22710, 13461, 21707, 19330, 23357, 10447, 9465],\n",
    "    '16704' : [1219, 1327, 20362, 1228, 55, 26980, 1236, 1247, 1365, 913],\n",
    "    '13702' : [864, 21079, 1111, 792, 1410, 8123, 8313, 1041, 1033, 1396]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('lab07s.json', 'w') as file:\n",
    "    json.dump(result, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
